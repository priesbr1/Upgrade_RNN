{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upgrade_RNN\n",
    "## Repository for IceCube-Upgrade RNN\n",
    "\n",
    "**Background**   \n",
    "This repository is for a Recurrent Neural Network (RNN) applied to simulated i3 data from the IceCube detector at the South Pole. The detector currently consists of 5,160 Digital Optical Modules (DOMs), each with one photomultiplier tube (PMT), situated on 86 strings between 1.5-2.5 km below the surface. The DOMs collect light emitted when a neutrino interacts with a nucleus in the ice, called an event. Depending on when, where, and how much light is detected, we can reconstruct properties of the neutrino like energy, zenith (vertical angle), and azimuth (horizontal angle), among other things. Each of these detections is called a hit.\n",
    "\n",
    "\n",
    "**Problem**   \n",
    "When neutrinos have a very high energy (usually $\\geq$ 1 TeV), they emit a lot of light in the detector, making it much easier to reconstruct its energy, direction, event type, etc. However The central area of IceCube, known as DeepCore, has a higher instrument density that the rest of the detector. DeepCore is used to probe the low-energy regime of neutrino physics, typically on the scale of $10^1$ GeV. This is where the IceCube-Upgrade comes in. The Upgrade is deploying 7 new stings, each with approximately 100 DOMs, into the DeepCore area. The Upgrade will also make use of two new DOM designs: the D-Egg with two PMTs (one on top, one on bottom), and the mDOM with 24 PMTs scattered over its surface. In total, Gen-1 and the Upgrade will have 15,700 PMTs. With these new DOMs they hope to improve energy reconstruction of low-energy events to the scale of $10^0$ GeV, as well as improve directional reconstruction.\n",
    "\n",
    "\n",
    "**i3 Data**   \n",
    "IceCube uses a specific data type referred to as \"i3.\" This data type is specific to IceCube and can be accessed using a module called I3Tray/IceTray. This module stores information about the event, as well as detector status, detector geometry.\n",
    "\n",
    "\n",
    "**RNN**   \n",
    "The RNN is neural network designed to handle data with a sequential (e.g. temporal) relationship, which is great for IceCube. The RNN takes in three input variables per event: a list of times when light was detected, a list of charges (proprotional to how much light was detected), and a list of generated IDs that describe which PMTs were triggered. Each of the corresponding entries in these lists (e.g. $t_1$, $q_1$, $p_1$) would comprise one hit, and all three lists comprise one event. The RNN outputs energy, dx/dy/dz direction, and error estimates for all four."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files\n",
    "\n",
    "**Attention.py:** Defines attention layer for RNN. No parsed inputs.\n",
    "\n",
    "**combine_hdf5.py:** Takes data in .hdf5 form and combines multiple files into a single file. Parsed inputs:   \n",
    "*input_files*: path/name of the input file(s) to be combined (can use `?` and `*` as wildcard fillers)   \n",
    "*output_files*: path/name of the output file after combination   \n",
    "\n",
    "**export_geometry.py:** Initializes and outputs Gen-1 detector geometry. No parsed inputs.\n",
    "\n",
    "**export_geometry_V5.py:** Initialzes and outputs Upgrade detector geometry. No parsed inputs.\n",
    "\n",
    "**filter_hdf5.py:** Applies cuts to .hdf5 data files and shuffles entries. Parsed inputs:   \n",
    "*input_file*: path/name of the input file that cuts are being applied to   \n",
    "*base_name*: starting point for the name of the output file (should be simple description of data, e.g. 'UpgradeNuMu')   \n",
    "*energy_range*: two-element list containing the minimum and maximum energy values for the output data   \n",
    "*keep_track*: whether or not to keep track events   \n",
    "*keep_cascade*: whether or not to keep cascade events   \n",
    "*keep_CC*: whether or not to keep CC (charged current) events   \n",
    "*keep_NC*: whether or not to keep NC (neutral current) events   \n",
    "*vertex_cut*: the type of vertex cut to apply   \n",
    "*flat_energy*: whether or not to flatten the energy distribution   \n",
    "*flat_tc*: whether or not to equalize the number of tracks/cascades   \n",
    "*min_PMT*: minimum number of triggered PMTs required for an event to pass   \n",
    "*max_events*: after all cuts, limit output file to specific number of events   \n",
    "*only_reco*: only keep events with PegLeg (likelihood-based) reconstructions   \n",
    "\n",
    "**Generators.py:** Sets up generators used to process large amounts of Gen-1 data for RNN. No parsed inputs.\n",
    "\n",
    "**Generators_V5.py:** Sets up generators used to process large amounts of Upgrade data for RNN. No parsed inputs.\n",
    "\n",
    "**i3_to_hdf5.py:** Processes information from Gen-1 .i3 files (or variants) into features/labels for the RNN, stored in .hdf5 format. Parsed inputs:   \n",
    "*file*: path/name of the input file(s) to be processed (can use `?` and `*` as wildcard fillers)   \n",
    "\n",
    "**i3_to_hdf5_V5.py:** Processes information from Upgrade .i3 files (or variants) into features/labels for the RNN, stored in .hdf5 format. Parsed inputs:   \n",
    "*file*: path/name of the input file(s) to be processed (can use `?` and `*` as wildcard fillers)   \n",
    "\n",
    "**MakePlots_RNN_V4.py:** Plots results from finished RNNs trained on Gen-1 data. Parsed inputs:   \n",
    "*hits*: maximum number of hits per event that the RNN utilizes   \n",
    "*epochs*: total number of trained epochs   \n",
    "*decay*: rate of learning rate decay   \n",
    "*lr*: inital learning rate   \n",
    "*dropout*: fraction of nodes whose weights will be dropped per epoch to prevent overtraining   \n",
    "*log_energy*: whether or not to use $log_{10}{energy}$ for training/testing   \n",
    "*file*: name of the input file used for training   \n",
    "*path*: path of the input file used for training   \n",
    "*output*: path of the output file for results   \n",
    "*standardize*: whether or not to standardize data   \n",
    "*checkpoints*: whether or not to use checkpoints from a previous training   \n",
    "*weights*: whether or not to use sample weights for training   \n",
    "\n",
    "**MakePlots_RNN_V5.py:** Plots results from finished RNNs trained on Upgrade data. Parsed inputs:   \n",
    "*hits*: maximum number of hits per event that the RNN utilizes   \n",
    "*epochs*: total number of trained epochs   \n",
    "*decay*: rate of learning rate decay   \n",
    "*lr*: inital learning rate   \n",
    "*dropout*: fraction of nodes whose weights will be dropped per epoch to prevent overtraining   \n",
    "*log_energy*: whether or not to use $log_{10}{energy}$ for training/testing   \n",
    "*file*: name of the input file used for training   \n",
    "*path*: path of the input file used for training   \n",
    "*output*: path of the output file for results   \n",
    "*standardize*: whether or not to standardize data   \n",
    "*checkpoints*: whether or not to use checkpoints from a previous training   \n",
    "*weights*: whether or not to use sample weights for training   \n",
    "\n",
    "**plot_inputoutput_datafile.py:** Plots distributions of input/output variables from Gen-1 data. Parsed inputs:   \n",
    "*hits*: maximum number of hits per event that the RNN utilizes   \n",
    "*epochs*: total number of trained epochs   \n",
    "*decay*: rate of learning rate decay   \n",
    "*lr*: inital learning rate   \n",
    "*dropout*: fraction of nodes whose weights will be dropped per epoch to prevent overtraining   \n",
    "*log_energy*: whether or not to use $log_{10}{energy}$ for training/testing   \n",
    "*file*: name of the input file used for training   \n",
    "*path*: path of the input file used for training   \n",
    "*output*: path of the output file for results   \n",
    "*standardize*: whether or not to standardize data   \n",
    "*checkpoints*: whether or not to use checkpoints from a previous training   \n",
    "*weights*: whether or not to use sample weights for training   \n",
    "*num_use*: maximum number of events to use for plotting   \n",
    "\n",
    "**plot_inputoutput_datafile_V5.py:** Plots distributions of input/output variables from Upgrade data. Parsed inputs:   \n",
    "*hits*: maximum number of hits per event that the RNN utilizes   \n",
    "*epochs*: total number of trained epochs   \n",
    "*decay*: rate of learning rate decay   \n",
    "*lr*: inital learning rate   \n",
    "*dropout*: fraction of nodes whose weights will be dropped per epoch to prevent overtraining   \n",
    "*log_energy*: whether or not to use $log_{10}{energy}$ for training/testing   \n",
    "*file*: name of the input file used for training   \n",
    "*path*: path of the input file used for training   \n",
    "*output*: path of the output file for results   \n",
    "*standardize*: whether or not to standardize data   \n",
    "*checkpoints*: whether or not to use checkpoints from a previous training   \n",
    "*weights*: whether or not to use sample weights for training   \n",
    "*num_use*: maximum number of events to use for plotting   \n",
    "\n",
    "**plot_inputoutput_RNN.py:** Plots distributions of input/output variables for an RNN trained on Gen-1 data. Parsed inputs:   \n",
    "*hits*: maximum number of hits per event that the RNN utilizes   \n",
    "*epochs*: total number of trained epochs   \n",
    "*decay*: rate of learning rate decay   \n",
    "*lr*: inital learning rate   \n",
    "*dropout*: fraction of nodes whose weights will be dropped per epoch to prevent overtraining   \n",
    "*log_energy*: whether or not to use $log_{10}{energy}$ for training/testing   \n",
    "*file*: name of the input file used for training   \n",
    "*path*: path of the input file used for training   \n",
    "*output*: path of the output file for results   \n",
    "*standardize*: whether or not to standardize data   \n",
    "*checkpoints*: whether or not to use checkpoints from a previous training   \n",
    "*weights*: whether or not to use sample weights for training   \n",
    "*num_use*: maximum number of events to use for plotting   \n",
    "\n",
    "**plot_inputoutput_RNN_V5.py:** Plots distributions of input/output variables for an RNN trained on Upgrade data. Parsed inputs:   \n",
    "*hits*: maximum number of hits per event that the RNN utilizes   \n",
    "*epochs*: total number of trained epochs   \n",
    "*decay*: rate of learning rate decay   \n",
    "*lr*: inital learning rate   \n",
    "*dropout*: fraction of nodes whose weights will be dropped per epoch to prevent overtraining   \n",
    "*log_energy*: whether or not to use $log_{10}{energy}$ for training/testing   \n",
    "*file*: name of the input file used for training   \n",
    "*path*: path of the input file used for training   \n",
    "*output*: path of the output file for results   \n",
    "*standardize*: whether or not to standardize data   \n",
    "*checkpoints*: whether or not to use checkpoints from a previous training   \n",
    "*weights*: whether or not to use sample weights for training   \n",
    "*num_use*: maximum number of events to use for plotting   \n",
    "\n",
    "**Plots.py:** Contains plotting functions used to plot variable distibutions and RNN results. No parsed inputs.\n",
    "\n",
    "**RNN_V4_2.py:** Initializes, trains, and tests an RNN on Gen-1 data, and plots results. Parsed inputs:   \n",
    "*hits*: maximum number of hits per event that the RNN utilizes   \n",
    "*epochs*: total number of trained epochs   \n",
    "*decay*: rate of learning rate decay   \n",
    "*lr*: inital learning rate   \n",
    "*dropout*: fraction of nodes whose weights will be dropped per epoch to prevent overtraining   \n",
    "*log_energy*: whether or not to use $log_{10}{energy}$ for training/testing   \n",
    "*file*: name of the input file used for training   \n",
    "*path*: path of the input file used for training   \n",
    "*output*: path of the output file for results   \n",
    "*standardize*: whether or not to standardize data   \n",
    "*checkpoints*: whether or not to use checkpoints from a previous training   \n",
    "*weights*: whether or not to use sample weights for training   \n",
    "\n",
    "**RNN_V5.py:** Initializes, trains, and tests an RNN on Upgrade data, and plots results. Parsed inputs:   \n",
    "*hits*: maximum number of hits per event that the RNN utilizes   \n",
    "*epochs*: total number of trained epochs   \n",
    "*decay*: rate of learning rate decay   \n",
    "*lr*: inital learning rate   \n",
    "*dropout*: fraction of nodes whose weights will be dropped per epoch to prevent overtraining   \n",
    "*log_energy*: whether or not to use $log_{10}{energy}$ for training/testing   \n",
    "*file*: name of the input file used for training   \n",
    "*path*: path of the input file used for training   \n",
    "*output*: path of the output file for results   \n",
    "*standardize*: whether or not to standardize data   \n",
    "*checkpoints*: whether or not to use checkpoints from a previous training   \n",
    "*weights*: whether or not to use sample weights for training   \n",
    "*data_type*: description of data used (should describe sample and cuts)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Pipeline\n",
    "\n",
    "**1a. i3_to_hdf5** (Gen-1 data):\n",
    "It first loads in an .i3 file (or variant), then sets up dictionaries for features, labels, and reco (LLH-based reco to compare to RNN results). It goes through every file in the list provided (or the single file if only one), then gets the information from every Physics frame in the file. It then grabs a cleaned/uncleaned pulseseries depending on the object provided. It saves the energy, zenith, azimuth, and x/y/z vertex position, as well as the LLH reco energy, zenith, and azimuth. It also saves the interaction type (CC/NC), event type (track/cascade), and the event weight. Then it creates lists for the `dom_index`, `pulse_time`, and `pulse_charge`. After that, it goes through every event in the pulseseries and saves a manually-generated DOM ID, and goes through every pulse in the event and saves the time and charge. It then shifts the time so that the average time for the entire dataset is 0. Lastly, it writes the ouput .hdf5 file using the same name as the input .i3 file.\n",
    "\n",
    "**1b. i3_to_hdf5** (Upgrade data):\n",
    "It first loads in an .i3 file (or variant), then sets up dictionaries for features, labels, and reco (LLH-based reco to compare to RNN results). It goes through every file in the list provided (or the single file if only one), then gets the information from every Physics frame in the file. It then grabs a cleaned/uncleaned pulseseries depending on the object provided. It saves the energy, zenith, azimuth, and x/y/z vertex position, as well as the LLH reco energy, zenith, and azimuth. It also saves the interaction type (CC/NC), event type (track/cascade, checking that the event is a neutrino interaction), and the event weight. Then it creates lists for the `pmt_index`, `pulse_time`, and `pulse_charge`. After that, it goes through every event in the pulseseries and saves a manually-generated PMT ID, and goes through every pulse in the event and saves the time and charge. It then shifts the time so that the average time for the entire dataset is 0. Lastly, it writes the ouput .hdf5 file using the same name as the input .i3 file.\n",
    "\n",
    "**2. combine_hdf5.py**\n",
    "It first loads the input files and check that they all exist and contain information. Next, it creates empty features/labels/reco dictionaries to match the data format of the input files. It then goes through all the non-empty input files and concatenates the information to the corresponding places in the output file dictionaries. As this happens, it also saves random selctions of some of the variables to check that they were properly stored. Lastly, it checks that these random portions of data or both correctly loaded from the input files and correctly saved in the output file.\n",
    "\n",
    "**3. filter_hdf5.py**\n",
    "It first prints out the selected cuts as a double-check that the right ones are being applied. It loads the information from the input file as NumPy arrays into separate dictionaries for easier manipulation. It then automatically names the output file based on the selected cuts, checking that certain cuts make sense, and shuffles all the data before beginning cuts. It applies cuts in the following order:   \n",
    "PMT, vertex, energy range, track/cascade, CC/NC, reco, energy flattening, track/cascade flattening, max events   \n",
    "After cuts, the data is shuffled again and it prints out the number of events before/after cuts. Lastly, it loads the data back into an .hdf5 file and saves it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
